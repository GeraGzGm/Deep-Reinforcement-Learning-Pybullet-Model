{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning on a CartPole made in Pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The main purpose of Deep Reinforcement Learning is about to train an \"agent\" that interacts with an \"environment\". The environment, represents the task or problem to be solved, in this case is the CartPole. And the agent, is the neural network that will be trained.\n",
    "\n",
    "![title](CartPend.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can obtain from the environment are 2 things:\n",
    "- State: It represents were our environment is at time t. For example, in this case, the state can be represented as an image, or as a state-vector that includes the position and velocity of the car($x,\\dot{x}$) and the angle, and angular velocity of the pendulum($\\theta,\\omega$).\n",
    "- Reward: Every time our environment does good things it will recieve a reward, and if it does something bad, will give it a penalty. Here, we'll reward the car in each time-step if the pole is between (-10°,10°),and if it goes out of the range we'll give it a penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent, as I mentioned, is going to be a NN, that will have 4 inputs and to outputs(to go left or right):\n",
    "![title](NN.png)\n",
    "\n",
    "The hidden neurons may change, but the input layer and output layer not.\n",
    "\n",
    "The purpose of the agent, is to maximize the future rewards , this is called \"policy\"(the strategy that the agent empolys to determine the next action based on the current state. It maps states into actions, actions that promises high rewards):$$\\sum_{t=0}^\\infty \\gamma^t R(x(t),a(t))  \\hspace{2cm} \\text{Bellman Equation}$$\n",
    "\n",
    "$\\gamma$ Is the \"discount factor\", its going to help us to give more importance to the actual reward than previus ones.\n",
    "\n",
    "We will need to create a matrix called \"Q\", this matrix maps state-action paits to the highest combination of immediate reward with all future rewards that might be harvested by later actions in the trayectory; this is the equation:\n",
    "$$Q(s_t,a_t) = (1-\\alpha) \\cdot Q(s_t,a_t) + \\eta \\cdot (R_t + \\gamma \\cdot \\max\\limits_{{a}} Q(s_{t+1},a))$$\n",
    "\n",
    "\n",
    "Here is a pseudocode from: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "![title](PseudoCode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import pybullet as pb\n",
    "import time\n",
    "from PIL import Image\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from packaging import version\n",
    "from datetime import datetime\n",
    "from tensorboard import notebook\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicsClient = pb.connect(pb.GUI)\n",
    "# physicsClient = pb.connect(pb.DIRECT)\n",
    "pb.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "pb.setGravity(0, 0, -9.81)\n",
    "pb.isNumpyEnabled()\n",
    "\n",
    "pb.setTimeStep(0.03)\n",
    "\n",
    "pb.configureDebugVisualizer(pb.COV_ENABLE_GUI,0)\n",
    "pb.configureDebugVisualizer(pb.COV_ENABLE_MOUSE_PICKING,0,pb.COV_ENABLE_VR_RENDER_CONTROLLERS,0)\n",
    "planetID = pb.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPend():\n",
    "    def __init__(self):\n",
    "        self.cubeStarsPos = [0,0,0.12]\n",
    "        self.cubeStartOrientation = pb.getQuaternionFromEuler([0,0,0])\n",
    "        \n",
    "        self.robotId = pb.loadURDF(\"cartpend.urdf\",self.cubeStarsPos,self.cubeStartOrientation)\n",
    "        \n",
    "        pb.setJointMotorControl2(self.robotId,4,pb.VELOCITY_CONTROL,force=0)\n",
    "        pb.resetJointState(self.robotId,4,-0.174 + (0.174+0.174)*np.random.rand())\n",
    "        \n",
    "        self.time = 0\n",
    "        self.reward = 0\n",
    "        self.End = False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.End = False\n",
    "        self.reward = 0\n",
    "        self.time = 0\n",
    "        \n",
    "    def run(self,userTorque):\n",
    "        \n",
    "        if userTorque == 0:\n",
    "            userTorque = -10\n",
    "        else:\n",
    "            userTorque = 10 \n",
    "        \n",
    "        for i in range(4):\n",
    "            pb.setJointMotorControl2(self.robotId,i,pb.VELOCITY_CONTROL,targetVelocity=userTorque)\n",
    "            \n",
    "        view = pb.getBasePositionAndOrientation(self.robotId)[0]\n",
    "        pb.resetDebugVisualizerCamera(3,0,-20,view)\n",
    "        pb.stepSimulation()\n",
    "        \n",
    "            \n",
    "        if abs(pb.getJointState(self.robotId,4)[0]) > 0.3 or abs(pb.getJointState(self.robotId,0)[0]) > 100:\n",
    "            self.reward -= 100\n",
    "            self.End = True\n",
    "        else:\n",
    "            self.reward += 1\n",
    "            self.reward -= pb.getJointState(self.robotId,0)[0]*0.01\n",
    "            self.time+=1\n",
    "        \n",
    "        time.sleep(0.01)\n",
    "        return pb.getJointState(self.robotId,0)[0:2] + pb.getJointState(self.robotId,4)[0:2],self.reward,self.End\n",
    "\n",
    "def Reset_Env(Env):\n",
    "    if Env.End:\n",
    "        time.sleep(0.1)\n",
    "        pb.removeBody(Env.robotId)\n",
    "        return CartPend()\n",
    "    return Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = 4\n",
    "        self.action_size = 2\n",
    "        self.EPISODES = 150\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.graph = []\n",
    "        \n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.batch_size = 128\n",
    "        self.train_start = 128\n",
    "\n",
    "        self.model = self.create_model()\n",
    "               \n",
    "    def create_model(self):\n",
    "        model = keras.models.Sequential()\n",
    "    \n",
    "        model.add(Dense(512, input_shape=(self.state_size,), activation=\"relu\"))\n",
    "        model.add(Dense(256, activation=\"relu\"))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dense(32, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory,self.batch_size)\n",
    "\n",
    "        action = [i[1] for i in minibatch]\n",
    "        reward = [i[2] for i in minibatch]\n",
    "        done = [i[4] for i in minibatch]\n",
    "        state = np.array([i[0] for i in minibatch]).reshape((self.batch_size,self.state_size))\n",
    "        next_state = np.array([i[3] for i in minibatch]).reshape((self.batch_size,self.state_size))\n",
    "        \n",
    "        \n",
    "        target = self.model.predict(state)\n",
    "        target_next = self.model.predict(next_state)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "             target[i][action[i]] = reward[i] if done[i] else reward[i] + self.gamma * (np.amax(target_next[i]))\n",
    "\n",
    "        self.model.fit(state, target, batch_size=self.batch_size, verbose=0,callbacks=[tensorboard_callback])\n",
    "        \n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name+\".h5\")\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name+\".h5\")\n",
    "              \n",
    "    def run(self):\n",
    "        print(\"Running\")\n",
    "        Env = CartPend()\n",
    "        \n",
    "        try: self.model.load_weights(\"CartPend_model.h5\")\n",
    "        except: pass\n",
    "        \n",
    "        for e in range(self.EPISODES):\n",
    "            state = pb.getJointState(Env.robotId,0)[0:2] + pb.getJointState(Env.robotId,4)[0:2]\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while True:\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done = Env.run(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    \n",
    "                self.memorize(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                \n",
    "                i += 1\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, self.EPISODES, Env.reward, self.epsilon))\n",
    "                    \n",
    "                    self.graph.append(Env.reward)\n",
    "                    \n",
    "                    if Env.reward >= 800:\n",
    "                        print(\"Saving trained model as cartpole-dqn.h5\")\n",
    "                        Agent.save(\"CartPend_model\")\n",
    "                        return\n",
    "                    Env = Reset_Env(Env)\n",
    "                    break\n",
    "                self.replay()    \n",
    "                \n",
    "        \n",
    "    def test(self):\n",
    "        Env = CartPend()\n",
    "        \n",
    "        self.model.load_weights(\"CartPend_model.h5\")\n",
    "        \n",
    "        for e in range(30):\n",
    "            state = pb.getJointState(Env.robotId,0)[0:2] + pb.getJointState(Env.robotId,4)[0:2]\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            done = False\n",
    "            i = 0\n",
    "            while True:\n",
    "                action = np.argmax(self.model.predict(state))\n",
    "                next_state, reward, done = Env.run(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    \n",
    "                state = next_state\n",
    "                \n",
    "                i += 1\n",
    "                if done:                   \n",
    "                    print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, 30, Env.reward, self.epsilon))\n",
    "                    Env = Reset_Env(Env)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/30, score: -41.412090882927906, e: 1.0\n",
      "episode: 1/30, score: -85.28798405230732, e: 1.0\n",
      "episode: 2/30, score: -85.28798403561373, e: 1.0\n",
      "episode: 3/30, score: -68.84906929327616, e: 1.0\n",
      "episode: 4/30, score: -86.24898658877369, e: 1.0\n",
      "episode: 5/30, score: -42.076143122189926, e: 1.0\n",
      "episode: 6/30, score: -8.97679581497114, e: 1.0\n",
      "episode: 7/30, score: 970.1882699588243, e: 1.0\n",
      "episode: 8/30, score: -87.21298958189452, e: 1.0\n",
      "episode: 9/30, score: -88.17999234042809, e: 1.0\n",
      "episode: 10/30, score: 115.14290555383329, e: 1.0\n",
      "episode: 11/30, score: -88.17999236403278, e: 1.0\n",
      "episode: 12/30, score: -88.1619942107773, e: 1.0\n",
      "episode: 13/30, score: -21.949266390077526, e: 1.0\n",
      "episode: 14/30, score: -87.21298966715158, e: 1.0\n",
      "episode: 15/30, score: -50.828229461226016, e: 1.0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-bb19d9252d3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mAgent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Agent.run()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mAgent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-6d9bc7f31d54>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                 \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-376d53a74147>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, userTorque)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetJointMotorControl2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobotId\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVELOCITY_CONTROL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargetVelocity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muserTorque\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetBasePositionAndOrientation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobotId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "Agent = DQNAgent()\n",
    "#Agent.run()\n",
    "Agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7012), started 0:37:23 ago. (Use '!kill 7012' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-645f5aaa137abf7d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-645f5aaa137abf7d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThisOne",
   "language": "python",
   "name": "thisone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
